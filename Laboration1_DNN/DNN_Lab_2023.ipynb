{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboration\n",
    "\n",
    "Data used in this laboration are from the Kitsune Network Attack Dataset, https://archive.ics.uci.edu/ml/datasets/Kitsune+Network+Attack+Dataset . We will focus on the 'Mirai' part of the dataset. Your task is to make a DNN that can classify if each attack is benign or malicious. The dataset has 116 covariates, but to make it a bit more difficult we will remove the first 24 covariates.\n",
    "\n",
    "You need to answer all questions in this notebook.\n",
    "\n",
    "If the training is too slow on your own computer, use the smaller datasets (*half or *quarter).\n",
    "\n",
    "Dense networks are not optimal for tabular datasets like the one used here, but here the main goal is to learn deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Get the data\n",
    "\n",
    "Skip this part if you load stored numpy arrays (Mirai*.npy) (which is recommended)\n",
    "\n",
    "Use `wget` in the terminal of your cloud machine (in the same directory as where you have saved this notebook) to download the data, i.e.\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_dataset.csv.gz\n",
    "\n",
    "wget https://archive.ics.uci.edu/ml/machine-learning-databases/00516/mirai/Mirai_labels.csv.gz\n",
    "\n",
    "Then unpack the files using `gunzip` in the terminal, i.e.\n",
    "\n",
    "gunzip Mirai_dataset.csv.gz\n",
    "\n",
    "gunzip Mirai_labels.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Get a graphics card\n",
    "\n",
    "Skip this part if you run on the CPU (recommended)\n",
    "\n",
    "Lets make sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming calculations in every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarning from numpy\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n",
    "\n",
    "# Allow growth of GPU memory, otherwise it will always look like all the memory is being used\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Hardware\n",
    "\n",
    "In deep learning, the computer hardware is very important. You should always know what kind of hardware you are working on. Lets pretend that everyone is using an Nvidia RTX 3090 graphics card.\n",
    "\n",
    "Question 1: Google the name of the graphics card, how many CUDA cores does it have?\n",
    "- 10 496\n",
    "\n",
    "Question 2: How much memory does the graphics card have?\n",
    "- 24 GB\n",
    "\n",
    "Question 3: What is stored in the GPU memory while training a DNN ?\n",
    "- training data and parameters(weights, outputs in each layer, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Load the data\n",
    "\n",
    "To make this step easier, directly load the data from saved numpy arrays (.npy) (recommended)\n",
    "\n",
    "\n",
    "Load the dataset from the csv files, it will take some time since it is almost 1.4 GB. (not recommended, unless you want to learn how to do it)\n",
    "\n",
    "We will use the function `genfromtxt` to load the data. (not recommended, unless you want to learn how to do it)\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "Load the data from csv files the first time, then save the data as numpy files for faster loading the next time.\n",
    "\n",
    "Remove the first 24 covariates to make the task harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Mirai_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Load data from numpy arrays, choose reduced files if the training takes too long\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mMirai_data.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mMirai_labels.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Remove the first 24 covariates (columns)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yj313\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Mirai_data.npy'"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt # Not needed if you load data from numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# Load data from numpy arrays, choose reduced files if the training takes too long\n",
    "X = np.load('Mirai_data.npy')\n",
    "Y = np.load('Mirai_labels.npy')\n",
    "\n",
    "\n",
    "\n",
    "# Remove the first 24 covariates (columns)\n",
    "\n",
    "X = X[:,24:]\n",
    "\n",
    "\n",
    "print('The covariates have size {}.'.format(X.shape))\n",
    "print('The labels have size {}.'.format(Y.shape))\n",
    "\n",
    "# Print the number of examples of each class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: How good is a naive classifier?\n",
    "\n",
    "Question 4: Given the number of examples from each class, how high classification performance can a naive classifier obtain? The naive classifier will assume that all examples belong to one class. Note: you do not need to make a naive classifier, this is a theoretical question, just to understand how good performance we can obtain by guessing that all examples belong to one class.\n",
    "\n",
    "In all classification tasks you should always ask these questions\n",
    "\n",
    "- How good classification accuracy can a naive classifier obtain? The naive classifier will assume that all examples belong to one class.\n",
    "- What is random chance classification accuracy if you randomly guess the label of each (test) example? For a balanced dataset and binary classification this is easy (50%), but in many cases it is more complicated and a Monte Carlo simulation may be required to estimate random chance accuracy.\n",
    "\n",
    "If your classifier cannot perform better than a naive classifier or a random classifier, you are doing something wrong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is common to have NaNs in the data, lets check for it. Hint: np.isnan()\n",
    "print(np.isnan(X).any())\n",
    "print(np.isnan(Y).any())\n",
    "\n",
    "# Print the number of NaNs (not a number) in the labels\n",
    "print(np.count_nonzero(np.isnan(X)))\n",
    "\n",
    "# Print the number of NaNs in the covariates\n",
    "print(np.count_nonzero(np.isnan(Y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Preprocessing\n",
    "\n",
    "Lets do some simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert covariates to floats\n",
    "X = X.astype(float)\n",
    "\n",
    "# Convert labels to integers\n",
    "Y = Y.astype(int)\n",
    "\n",
    "# Remove mean of each covariate (column)\n",
    "for col in range(X.shape[1]):\n",
    "    mean = np.mean(X[:,col])\n",
    "    for row in range(X.shape[0]):\n",
    "        X[row, col] -= mean\n",
    "\n",
    "\n",
    "# Divide each covariate (column) by its standard deviation\n",
    "for col in range(X.shape[1]):\n",
    "    stdev = np.std(X[:,col])\n",
    "    for row in range(X.shape[0]):\n",
    "        X[row, col] /= stdev\n",
    "\n",
    "# Check that mean is 0 and standard deviation is 1 for all covariates, by printing mean and std\n",
    "meanList = []\n",
    "stdevList = []\n",
    "for col in range(X.shape[1]):\n",
    "    meanList.append(np.mean(X[:,col]))\n",
    "    stdevList.append(np.std(X[:,col]))\n",
    "\n",
    "print(meanList)\n",
    "print(stdevList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Split the dataset\n",
    "\n",
    "Use the first 70% of the dataset for training, leave the other 30% for validation and test, call the variables\n",
    "\n",
    "Xtrain (70%)\n",
    "\n",
    "Xtemp  (30%)\n",
    "\n",
    "Ytrain (70%)\n",
    "\n",
    "Ytemp  (30%)\n",
    "\n",
    "We use a function from scikit learn.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code to split the dataset\n",
    "Xtrain, Xtemp, Ytrain, Ytemp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print('Xtrain has size {}.'.format(Xtrain.shape))\n",
    "print('Ytrain has size {}.'.format(Ytrain.shape))\n",
    "\n",
    "print('Xtemp has size {}.'.format(Xtemp.shape))\n",
    "print('Ytemp has size {}.'.format(Ytemp.shape))\n",
    "\n",
    "# Print the number of examples of each class, for the training data and the remaining 30%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Part 8: Split non-training data data into validation and test\n",
    "Now split your non-training data (Xtemp, Ytemp) into 50% validation (Xval, Yval) and 50% testing (Xtest, Ytest), we use a function from scikit learn. In total this gives us 70% for training, 15% for validation, 15% for test.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Do all variables (Xtrain,Ytrain), (Xval,Yval), (Xtest,Ytest) have the shape that you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code\n",
    "Xval, Xtest, Yval, Ytest = train_test_split(Xtemp, Ytemp, test_size=0.5, random_state=42)\n",
    "\n",
    "print('The validation and test data have size {}, {}, {} and {}'.format(Xval.shape, Xtest.shape, Yval.shape, Ytest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: DNN classification\n",
    "\n",
    "Finish this code to create a first version of the classifier using a DNN. Start with a simple network with 2 dense layers (with 20 nodes each), using sigmoid activation functions. The final dense layer should have a single node and a sigmoid activation function. We start with the SGD optimizer.\n",
    "\n",
    "For different parts of this notebook you need to go back here, add more things, and re-run this cell to re-define the build function.\n",
    "\n",
    "Relevant functions are\n",
    "\n",
    "`model.add()`, adds a layer to the network\n",
    "\n",
    "`Dense()`, a dense network layer\n",
    "\n",
    "`model.compile()`, compile the model, add \" metrics=['accuracy'] \" to print the classification accuracy during the training\n",
    "\n",
    "See https://keras.io/layers/core/ for information on how the `Dense()` function works\n",
    "\n",
    "Import a relevant cost / loss function for binary classification from keras.losses (https://keras.io/losses/)\n",
    "\n",
    "See the following links for how to compile, train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#compile-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n",
    "\n",
    "Make sure that the last layer always has a sigmoid activation function (why?).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# Set seed from random number generator, for better comparisons\n",
    "from numpy.random import seed\n",
    "seed(123)\n",
    "\n",
    "def build_DNN(input_shape, n_layers, n_nodes, act_fun='sigmoid', optimizer='sgd', learning_rate=0.01, \n",
    "              use_bn=False, use_dropout=False, use_custom_dropout=False):\n",
    "    \n",
    "    # Setup optimizer, depending on input parameter string\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        optimizer = Adam(learning_rate=0.1)\n",
    "\n",
    "    # Setup a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers to the model, using the input parameters of the build_DNN function\n",
    "\n",
    "    \n",
    "    # Add first layer, requires input shape\n",
    "    model.add(Input(shape=(input_shape[1],)))\n",
    "\n",
    "    \n",
    "    \n",
    "    # Add remaining layers, do not require input shape\n",
    "    for i in range(n_layers,):\n",
    "        if use_bn == False:\n",
    "            model.add(Dense(n_nodes, activation=act_fun))\n",
    "        if use_bn == True:\n",
    "            model.add(Dense(n_nodes))\n",
    "            model.add(Activation(act_fun))\n",
    "            model.add(BatchNormalization())\n",
    "\n",
    "            if use_dropout == True:\n",
    "                if use_dropout < 1 and use_dropout>0 :\n",
    "                    model.add(Dropout(rate=use_dropout))\n",
    "            else:\n",
    "                model.add(Dropout( rate=0.5))\n",
    "                    \n",
    "            if use_custom_dropout:\n",
    "                if use_custom_dropout == True:\n",
    "                    use_custom_dropout = 0.5\n",
    "                model.add(myDropout(use_custom_dropout))\n",
    "\n",
    "\n",
    "    \n",
    "    # Add final layer\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a help function for plotting the training results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_results(history):\n",
    "    \n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.plot(acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['Training','Validation'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Train the DNN\n",
    "\n",
    "Time to train the DNN, we start simple with 2 layers with 20 nodes each, learning rate 0.1.\n",
    "\n",
    "Relevant functions\n",
    "\n",
    "`build_DNN`, the function we defined in Part 9, call it with the parameters you want to use\n",
    "\n",
    "`model.fit()`, train the model with some training data\n",
    "\n",
    "`model.evaluate()`, apply the trained model to some test data\n",
    "\n",
    "See the following links for how to train and evaluate the model\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#fit-method\n",
    "\n",
    "https://keras.io/api/models/model_training_apis/#evaluate-method\n",
    "\n",
    "\n",
    "Make sure that you are using learning rate 0.1 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build the model\n",
    "model1 = build_DNN(input_shape, n_layers=2, n_nodes=20)\n",
    "\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, provide training data and validation data7\n",
    "history1 = model1.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "score = model1.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the history from the training run\n",
    "plot_results(history1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11: More questions\n",
    "\n",
    "Question 5: What happens if you add several Dense layers without specifying the activation function?\n",
    "\n",
    "- with no activation function, the NN will be linear and make extra dense layers cannot improve performance at all.\n",
    "\n",
    "Question 6: How are the weights in each dense layer initialized as default? How are the bias weights initialized?\n",
    "\n",
    "- with default setting of dense() (kernel_initializer=\"glorot_uniform\",bias_initializer=\"zeros\",), the weight is intialized by glorot_uniform(Xavier uniform initializer) and the bias is initialized by zeros.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12: Balancing the classes\n",
    "\n",
    "This dataset is rather unbalanced, we need to define class weights so that the training pays more attention to the class with fewer samples. We use a function in scikit learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "\n",
    "You need to call the function something like this\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight = , classes = , y = )\n",
    "\n",
    "otherwise it will complain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(Y), y=Ytrain)\n",
    "\n",
    "# Print the class weights\n",
    "print(class_weights)\n",
    "\n",
    "# Keras wants the weights in this form, uncomment and change value1 and value2 to your weights, \n",
    "# or get them from the array that is returned from class_weight\n",
    "\n",
    "class_weights = {0: class_weights[0],\n",
    "                1: class_weights[1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model2 = build_DNN(input_shape, n_layers=2, n_nodes=20)\n",
    "\n",
    "history2 = model2.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model2.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 13: More questions\n",
    "\n",
    "Skip questions 8 and 9 if you run on the CPU (recommended)\n",
    "\n",
    "Question 7: Why do we have to use a batch size? Why can't we simply use all data at once? This is more relevant for even larger datasets.\n",
    "\n",
    "- if the dataset is too large( compared with the memories we are using), it cannot be stored in memories if we don't divide it into smaller batches to fit the memory size.\n",
    "\n",
    "Question 8: How busy is the GPU for a batch size of 100? How much GPU memory is used? Hint: run 'nvidia-smi' on the computer a few times during training.\n",
    "\n",
    "Question 9: What is the processing time for one training epoch when the batch size is 100? What is the processing time for one epoch when the batch size is 1,000? What is the processing time for one epoch when the batch size is 10,000? Explain the results. \n",
    "\n",
    "Question 10: How many times are the weights in the DNN updated in each training epoch if the batch size is 100? How many times are the weights in the DNN updated in each training epoch if the batch size is 1,000? How many times are the weights in the DNN updated in each training epoch if the batch size is 10,000?  \n",
    "\n",
    "- Number of weight updates per epoch = number of training examples / batch size, so less times of weights updating will be performed when batch size grows\n",
    "\n",
    "Question 11: What limits how large the batch size can be?\n",
    "\n",
    "- the memoey size.\n",
    "\n",
    "Question 12: Generally speaking, how is the learning rate related to the batch size? If the batch size is decreased, how should the learning rate be changed?\n",
    "\n",
    "- when batch size increases, the times of weight updating will reduce for each epoch. Thus we will need a greater learning rate due to fewer weight updates in same epochs.\n",
    "\n",
    "\n",
    "Lets use a batch size of 10,000 from now on, and a learning rate of 0.1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 14: Increasing the complexity\n",
    "\n",
    "Lets try some different configurations of number of layers and number of nodes per layer.\n",
    "\n",
    "Question 13: How many trainable parameters does the network with 4 dense layers with 50 nodes each have, compared to the initial network with 2 layers and 20 nodes per layer? Hint: use model.summary()\n",
    "\n",
    "- there will be 1860+420+21 = 2301 parameters in our 2 dense layer model and 4650 + 3*2550 +51 = 12351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14 = build_DNN(input_shape, n_layers=4, n_nodes=50)\n",
    "model14.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 20 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model3 = build_DNN(input_shape, n_layers=4, n_nodes=20)\n",
    "\n",
    "history3 = model3.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model3.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model4 = build_DNN(input_shape, n_layers=2, n_nodes=50)\n",
    "\n",
    "history4 = model4.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model4.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers, 50 nodes, class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model5 = build_DNN(input_shape, n_layers=4, n_nodes=50)\n",
    "\n",
    "history5 = model5.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model5.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 15: Batch normalization\n",
    "\n",
    "Now add batch normalization after each dense layer in `build_DNN`. Remember to import BatchNormalization from keras.layers. \n",
    "\n",
    "See https://keras.io/layers/normalization/ for information about how to call the function.\n",
    "\n",
    "Question 14: Why is batch normalization important when training deep networks?\n",
    "\n",
    "- Normalizing the input makes optimization easier, since the loss function behaves nicer (more isotropic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model6 = build_DNN(input_shape, n_layers=2, n_nodes=20, use_bn = True)\n",
    "\n",
    "history6 = model6.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model6.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 16: Activation function\n",
    "\n",
    "Try changing the activation function in each layer from sigmoid to ReLU, write down the test accuracy.\n",
    "\n",
    "Note: the last layer should still have a sigmoid activation function.\n",
    "\n",
    "https://keras.io/api/layers/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, ReLU, no batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model7 = build_DNN(input_shape, n_layers=2, n_nodes=20, act_fun = 'relu')\n",
    "\n",
    "history7 = model7.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model7.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 17: Optimizer\n",
    "\n",
    "Try changing the optimizer from SGD to Adam (with learning rate 0.1 as before). Remember to import the Adam optimizer from keras.optimizers. \n",
    "\n",
    "https://keras.io/optimizers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, Adam optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model8 = build_DNN(input_shape, n_layers=2, n_nodes=20, optimizer='adam')\n",
    "\n",
    "history8 = model8.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model8.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 18: Dropout regularization\n",
    "\n",
    "Dropout is a type of regularization that can improve accuracy for validation and test data. It randomly removes connections to force the neural network to not rely too much on a small number of weights.\n",
    "\n",
    "Add a Dropout layer after each Dense layer (but not after the final dense layer) in `build_DNN`, with a dropout probability of 50%. Remember to first import the Dropout layer from keras.layers\n",
    "\n",
    "See https://keras.io/api/layers/regularization_layers/dropout/ for how the Dropout layer works.\n",
    "\n",
    "---\n",
    "\n",
    "Question 15: How does the validation accuracy change when adding dropout?\n",
    "\n",
    "After adding dropout, the validation accuracy decrease first and then increase in most cases.\n",
    "\n",
    "Question 16: How does the test accuracy change when adding dropout?\n",
    "\n",
    "The test accuracy will increase, expecially the original mode is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 layers, 20 nodes, class weights, dropout, SGD optimizer, no batch normalization, sigmoid activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup some training parameters\n",
    "batch_size = 10000\n",
    "epochs = 20\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model9 = build_DNN(input_shape, n_layers=2, n_nodes=20 ,use_dropout=True)\n",
    "\n",
    "history9 = model9.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "score = model9.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(history9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 19: Improving performance\n",
    "\n",
    "Spend some time (30 - 90 minutes) playing with the network architecture (number of layers, number of nodes per layer, activation function) and other hyper parameters (optimizer, learning rate, batch size, number of epochs, degree of regularization). For example, try a much deeper network. How much does the training time increase for a network with 10 layers?\n",
    "\n",
    "Question 17: How high classification accuracy can you achieve for the test data? What is your best configuration?\n",
    "- Test accuracy: 0.9426. We added more epochs(40) and layers(10) with nodes(80) for a better performance, and use a 0.7 dropout rate to mitigate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find your best configuration for the DNN\n",
    "\n",
    "batch_size = 10000\n",
    "epochs = 50\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train DNN\n",
    "model10 = build_DNN(input_shape, n_layers=3, n_nodes=80 ,use_dropout=0.7, use_bn=True)\n",
    "\n",
    "history10 = model10.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size,validation_data=(Xval,Yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DNN on test data\n",
    "score = model10.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "\n",
    "print('Test loss: %.4f' % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "plot_results(history10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 20: Dropout uncertainty\n",
    "\n",
    "Dropout can also be used during testing, to obtain an estimate of the model uncertainty. Since dropout will randomly remove connections, the network will produce different results every time the same (test) data is put into the network. This technique is called Monte Carlo dropout. For more information, see this paper http://proceedings.mlr.press/v48/gal16.pdf\n",
    "\n",
    "To achieve this, we need to redefine the Keras Dropout call by running the cell below, and use 'myDropout' in each call to Dropout, in the cell that defines the DNN. The `build_DNN` function takes two boolean arguments, use_dropout and use_custom_dropout, add a standard Dropout layer if use_dropout is true, add a myDropout layer if use_custom_dropout is true.\n",
    "\n",
    "Run the same test data through the trained network 100 times, with dropout turned on. \n",
    "\n",
    "Question 18: What is the mean and the standard deviation of the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "\n",
    "class myDropout(keras.layers.Dropout):\n",
    "    \"\"\"Applies Dropout to the input.\n",
    "    Dropout consists in randomly setting\n",
    "    a fraction `rate` of input units to 0 at each update during training time,\n",
    "    which helps prevent overfitting.\n",
    "    # Arguments\n",
    "        rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "        noise_shape: 1D integer tensor representing the shape of the\n",
    "            binary dropout mask that will be multiplied with the input.\n",
    "            For instance, if your inputs have shape\n",
    "            `(batch_size, timesteps, features)` and\n",
    "            you want the dropout mask to be the same for all timesteps,\n",
    "            you can use `noise_shape=(batch_size, 1, features)`.\n",
    "        seed: A Python integer to use as random seed.\n",
    "    # References\n",
    "        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\n",
    "           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, rate, training=True, noise_shape=None, seed=None, **kwargs):\n",
    "        super(myDropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)\n",
    "        self.training = training\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed)\n",
    "            if not training: \n",
    "                return K.in_train_phase(dropped_inputs, inputs, training=self.training)\n",
    "            return K.in_train_phase(dropped_inputs, inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your best config, custom dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your best training parameters\n",
    "# the previous best training parameters took too much time so we change it a little bit\n",
    "batch_size = 10000\n",
    "epochs = 40\n",
    "input_shape = X.shape\n",
    "\n",
    "# Build and train model\n",
    "model11 = build_DNN(input_shape, n_layers=2, n_nodes=50, use_custom_dropout = 0.7, use_bn = True)\n",
    "\n",
    "history11 = model11.fit(Xtrain, Ytrain, epochs=epochs, batch_size=batch_size, validation_data=(Xval,Yval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times to evalute the model on test data, \n",
    "# if you get slightly different test accuracy every time, Dropout during testing is working\n",
    "\n",
    "# Evaluate model on test data\n",
    "score = model11.evaluate(Xtest,Ytest, batch_size=batch_size)\n",
    "                       \n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "print('crossentropy: %.4f' % score[0])\n",
    "\n",
    "plot_results(history11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the testing 100 times, and save the accuracies in an array\n",
    "accuracy = []\n",
    "for i in range(100):\n",
    "    score = model11.evaluate(Xtest,Ytest, batch_size=batch_size, verbose=0)\n",
    "    accuracy.append(score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate and print mean and std of accuracies\n",
    "print(f\"mean = {np.mean(accuracy)}\" )\n",
    "print(f\"std = {np.std(accuracy)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 21: Cross validation uncertainty\n",
    "\n",
    "Cross validation (CV) is often used to evaluate a model, by training and testing using different subsets of the data it is possible to get the uncertainty as the standard deviation over folds. We here use a help function from scikit-learn to setup the CV, see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html . Use 10 folds with shuffling, random state 1234. \n",
    "\n",
    "Note: We here assume that you have found the best hyper parameters, so here the data are only split into training and testing, no validation.\n",
    "\n",
    "---\n",
    "\n",
    "Question 19: What is the mean and the standard deviation of the test accuracy?\n",
    "\n",
    "Question 20: What is the main advantage of dropout compared to CV for estimating test uncertainty? The difference may not be so large in this notebook, but imagine that you have a network that takes 24 hours to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define 10-fold cross validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1234)\n",
    "\n",
    "accuracy=[]\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, Y)):\n",
    "# Loop over cross validation folds\n",
    "    Xtrain = X[train_index,:]\n",
    "    Ytrain = Y[train_index]\n",
    "    Xtest = X[test_index,:]\n",
    "    Ytest = Y[test_index]\n",
    "    # Calculate class weights for current split\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(Y), y=Ytrain)\n",
    "    class_weights = {0: class_weights[0],\n",
    "                1: class_weights[1]}\n",
    "    # Rebuild the DNN model, to not continue training on the previously trained model\n",
    "    batch_size = 10000\n",
    "    epochs = 20\n",
    "    input_shape = X.shape\n",
    "    modelKF = build_DNN(input_shape, n_layers=2, n_nodes=20)\n",
    "    \n",
    "    # Fit the model with training set and class weights for this fold\n",
    "    historyKF = modelKF.fit(Xtrain, Ytrain, verbose = 0, epochs=epochs, batch_size=batch_size, validation_data=(Xval,Yval), class_weight=class_weights)\n",
    "    \n",
    "    # Evaluate the model using the test set for this fold\n",
    "    score = modelKF.evaluate(Xtest,Ytest, batch_size=batch_size, verbose = 0)\n",
    "    # Save the test accuracy in an array\n",
    "    accuracy.append(score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print mean and std of accuracies\n",
    "print(f\"mean = {np.mean(accuracy)}\" )\n",
    "print(f\"std = {np.std(accuracy)}\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 22: DNN regression\n",
    "\n",
    "A similar DNN can be used for regression, instead of classification.\n",
    "\n",
    "Question 21: How would you change the DNN used in this lab in order to use it for regression instead?\n",
    "- make the output linear(with no activation) to make possible output have a range of (-inf, inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report\n",
    "\n",
    "Send in this jupyter notebook, with answers to all questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
